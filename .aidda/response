File: /home/stevegt/lab/grid-cli/v2/doc/003-design.md
```
## Cache and Module Handling in the PromiseGrid Kernel

### Introduction

In the PromiseGrid Kernel, caching and module handling are central to optimizing and managing the execution and storage of data. This document discusses the strategies and implementation details of these mechanisms, highlighting their alignment with decentralized governance and microkernel architecture principles.

### Cache Structures

1. **Directory Tree Structure**:
    - The cache **SHOULD BE** a directory tree rather than an in-memory map. This approach ensures persistent storage and efficient management of large datasets.
    - Cache keys **MUST** use filesystem separators (`/`) between each key component, and arguments **MUST** be URL-encoded when building the cache key to handle special characters safely.

2. **Multiple Caches**:
    - There **MAY BE** multiple caches, including the built-in cache in the kernel and caches provided by various modules.
    - The kernel **SHOULD** load the built-in cache from embedded resources using Go's `embed` feature.
    - The kernel **MAY** use the Origin Private File System (OPFS) for disk file access and **MAY** utilize the `afero` library to abstract filesystem interactions.

### Treating Modules as Caches

1. **Role of Modules**:
    - The kernel **MUST** treat modules as caches. In the event of a cache miss, the kernel **MUST** consult one or more modules to retrieve the requested value.
    - Modules **MAY** contribute to the cache or provide the requested data dynamically.

2. **Unified Interface**:
    - From the caller's perspective, there **SHALL BE** no difference between a cache lookup and a function call. Both operations **SHALL BE** treated as hashed function calls. The caller sends a message with a leading hash and any arguments and receives a message containing the requested content.

### Acceptance and Promises

1. **Acceptance Criteria**:
    - Modules **MUST** define acceptance criteria for promises, module hashes, and arguments into a single function, `Accept()`.
    - By returning a promise message from `Accept` instead of a boolean, modules provide additional guarantees and meta-information.

2. **Promises as Acceptance**:
    - The `Accept` function returns a promise message that includes whether the module accepts the request and any relevant metadata. If `HandleMessage` fails, it is considered a broken promise.

3. **Ant Routing Mechanism**:
    - The syscall tree acts as an "ant routing" mechanism, caching successful paths to optimize future routing. In the future, similar calls follow the same path to the same module.
    - The syscall tree **MUST** use hierarchical keys and **SHOULD** filter based on whether any module accepts the leading parameters, matches the module hash, and accepts the arguments.

### Integration with Church, Turing, and Chomsky's Concept of "Accept"

1. **Computational Theory**:
    - The term "accept" aligns with Church, Turing, and Chomsky's use in computing theory and languages, where an automaton or machine accepts an input if it reaches an accepting state.
    - In PromiseGrid, modules act as recognizers for specific tasks, based on the promise hash, module hash, and arguments.

2. **Promises All the Way Down**:
    - The concept of "promises all the way down" integrates acceptance as a promise message, enhancing robustness and trustworthiness.
    - Each layer (modules, syscall tree, kernel) makes and fulfills promises based on the promises made by the layers below it.

### Kernel's Dynamic Syscall Tree

1. **Syscall Table and Acceptance History**:
    - The kernel **SHOULD** use a dynamic syscall table to store positive and negative acceptance history for all modules.
    - This table **SHOULD** start empty and be populated during operation as the kernel consults built-in and other modules to handle received messages.
    - The kernel **MUST** route messages to the module whose syscall table key matches the most leading parameters components, optimizing routing and reducing redundant checks.

### Combining Accept and HandleMessage Functions

#### Pros and Cons of Separate `Accept()` and `HandleMessage()` Functions versus a Single Function

##### Combined Function:

1. **Simplicity**: Combines decision-making and handling into a single function.
2. **Consistency**: Ensures the decision to handle and the actual handling are tightly coupled, increasing consistency and reducing logic duplication.
3. **Efficiency**: Eliminates redundant checks by combining acceptance and handling.

##### Separate Functions:

1. **Clarity**: Maintains clear separation between decision-making and handling logic.
2. **Early Rejection**: Allows for quick rejection of messages based on promise, module hash, or arguments without performing any handling.
3. **Modular Logic**: Facilitates modular and specialized acceptance and handling logic.

### Acceptance as Promise

Combining the `Accept` and `HandleMessage` functions would imply that the acceptance itself is a promise, aligning with the concept that "it's promises all the way down." This structure implies that:

1. **Acceptance as a Promise**: The first element in `Parms` indicates whether the module promises to handle the message.
2. **Handling as Promise Fulfillment**: `HandleMessage` attempts to fulfill this promise. If it fails (breaking the promise), it is logged and handled appropriately.

### Proposed Dynamic Syscall Table and Ant Routing

Implementing a dynamic syscall table with ant routing involves:

1. **Hierarchical Syscall Tree**: Each node in the tree can have multiple children representing different components of the parameters (`parms`).
2. **Caching Successful Paths**: Cache successful paths to optimize future routing based on leading parameter sequences.
3. **Populating During Operation**: The syscall tree starts empty and is populated during operation as modules accept and handle messages.

### Examples of Unified and Separate Function Approaches

**Combined Function:

```go
type Module interface {
    ProcessMessage(ctx context.Context, parms ...interface{}) (Message, error)
}

type LocalCacheModule struct {
    cacheDir string
}

func NewLocalCacheModule(cacheDir string) *LocalCacheModule {
    return &LocalCacheModule{cacheDir: cacheDir}
}

func (m *LocalCacheModule) ProcessMessage(ctx context.Context, parms ...interface{}) (Message, error) {
    // Implement logic to process message, which includes accepting and handling
    // Return a promise message with acceptance and handling results or errors
}
```

**Separate Functions:

```go
type Module interface {
    Accept(ctx context.Context, parms ...interface{}) (Message, error)
    HandleMessage(ctx context.Context, parms ...interface{}) ([]byte, error)
}

type LocalCacheModule struct {
    cacheDir string
}

func NewLocalCacheModule(cacheDir string) *LocalCacheModule {
    return &LocalCacheModule{cacheDir: cacheDir}
}

func (m *LocalCacheModule) Accept(ctx context.Context, parms ...interface{}) (Message, error) {
    // Implement logic to accept or reject based on parms
}

func (m *LocalCacheModule) HandleMessage(ctx context.Context, parms ...interface{}) ([]byte, error) {
    // Implement logic to handle the message following acceptance
}
```

### Conclusion

By integrating promises at every level and implementing a hierarchical syscall tree with caching and acceptance history, PromiseGrid ensures trust, accountability, and efficient message handling in a decentralized governance framework. The simplified message structure and consistent handling of cache and modules contribute to a robust and flexible system.

Here **MAY BE** multiple caches, including the built-in cache in the kernel and caches provided by various modules.

2. **Integration with Modules**:
   - The kernel **MUST** treat modules as caches. In the event of a cache miss, the kernel **MUST** consult one or more modules to retrieve the requested value.
   - The kernel **SHOULD** load the built-in cache from embedded resources using Go’s embed feature.
   - From the caller's perspective, there **SHALL BE** no difference between a cache lookup and a function call. Both operations **SHALL BE** treated as byte sequence completions.

### Network Communications

**Are network communications always done through modules, or can the kernel do some of that itself?**

Network communications in the PromiseGrid Kernel model are primarily intended to be handled by modules rather than the kernel itself. This design aligns with the microkernel architecture principles, where the kernel provides minimal core functionalities and delegates most tasks, such as network communication, to service modules. The kernel’s role includes managing module execution, handling inter-process communication (IPC), and maintaining security and resource control.

### Capability Tokens and Encoding

Several systems and frameworks implement self-contained capability tokens:

1. **Macaroons**: These are flexible authorization credentials that support delegation and attenuation. A macaroon is a bearer token that can encapsulate permissions and is augmented with caveats, which are conditions that must be satisfied for the macaroon to be considered valid.
2. **JSON Web Tokens (JWT)**: JWTs are widely used in web applications to assert claims between parties. They can include custom claims to specify permissions and the intended audience. A JWT can be signed to ensure authenticity and integrity.
3. **Caveats in Capability-Based Systems**: Traditional capability-based security systems sometimes support a form of caveats or restrictions within the tokens themselves to specify the scope of permissions and the authorized user.

### Open Questions

- How can we best handle broken promises effectively?
- What are the most efficient methods to determine the best route when multiple handlers are available?
- How should we reconcile byte sequence completion with explicit registration and hash-based routing?

## Hybrid DHT and Open Market Design

### Introduction

This document outlines a design concept that combines aspects of a Distributed Hash Table (DHT) with an open market system. This hybrid design aims to leverage the decentralized storage and lookup efficiency of DHTs while incorporating the dynamic and competitive aspects of an open market. The goal is to create a more versatile and resilient data storage and retrieval system.

### Core Components

1. **DHT Backbone**
   - Distributed Hash Table (DHT) forms the primary underlying structure for decentralized storage and data retrieval.
   - Each node in the DHT is responsible for a portion of the key space.
   - Nodes store key-value pairs and use consistent hashing to manage the distribution of data.

2. **Open Market Layer**
   - An open market layer functions atop the DHT, allowing nodes to trade storage and retrieval services.
   - Nodes can offer their storage capacity and bandwidth in exchange for compensation.
   - Market dynamics, such as supply, demand, pricing, and competition, determine the cost and allocation of resources.

### Design Overview

#### DHT Backbone

##### Structure

- **Nodes and Keys:** Every node in the system holds a unique identifier. Keys are distributed across the network based on consistent hashing.
- **Routing:** Nodes maintain routing tables with information about a subset of other nodes, allowing efficient key lookups with a logarithmic number of hops.
- **Replication:** Data is replicated across multiple nodes to ensure availability and fault tolerance. Replication factors can be influenced by market supply and demand dynamics; a node can decide on its own to keep and offer a copy if it sees a price increase.

#### Pros and Cons of Including the DHT Layer

**Pros:**
- **Decentralization:** The DHT layer ensures that data storage and retrieval are distributed, reducing the reliance on central servers.
- **Scalability:** Due to its logarithmic complexity in lookups, the DHT structure can scale well with the increase in the number of nodes.
- **Fault Tolerance:** Replication across multiple nodes increases data availability and resilience against node failures.

**Cons:**
- **Complexity:** Implementing a hybrid model that combines DHT and open market dynamics can be complex and might require sophisticated mechanisms for balancing the two.
- **Resource Intensity:** Maintaining a DHT involves overhead in terms of maintaining routing tables and managing data replication, which can be resource-intensive.
- **Security Risks:** While decentralized, a DHT can still be vulnerable to certain types of attacks like sybil attacks and requires robust security mechanisms.

#### Open Market Layer

##### Market Mechanics

- **Storage Offers:** Nodes can advertise their available storage capacity. Each offer includes terms such as price, duration, and conditions.
- **Bidding:** Nodes in need of storage can place bids, proposing terms for storing their data. The market matches offers and bids based on compatibility and preferences.
- **Dynamic Pricing:** Prices for storage and retrieval services are dynamic, influenced by demand, supply, and competition among nodes.

##### Transactions

- **Promises and Capabilities:** Burgess-style promises and security-token-style capabilities replace traditional contracts, rewards, and penalties.
- **Broken Promise Accounting:** Enforcement mechanisms are replaced with broken promise accounting, which impacts a node's reputation within the system.

### Data Storage and Retrieval

#### Storage Process

1. **Identification:** The data to be stored is assigned a unique key using a cryptographic hash function.
2. **Market Interaction:** The node seeking storage interacts with the market layer, finding a suitable storage offer.
3. **Promise Formation:** A promise is formed and agreed upon by both parties, detailing the storage terms.
4. **Data Chunking:** The data is divided into chunks using Rabin polynomials (if necessary) and distributed to the selected storage nodes according to the promise.
5. **Replication:** The data is replicated across multiple nodes to ensure reliability as per the agreed replication factor.

#### Retrieval Process

1. **Identification:** The key associated with the data is used to locate the data across the DHT.
2. **Market Interaction:** The node seeking to retrieve the data interacts with the market to find retrieval offers.
3. **Promise Formation:** A retrieval promise is established, detailing the retrieval terms.
4. **Data Transfer:** The data is retrieved from the storage nodes and transferred to the requesting node according to the promise conditions.

#### Node Advertising for Blob Storage

Even though the DHT handles primary routing and replication, a node that chooses to store a blob copy despite DHT routing can advertise this copy through the open market layer. The steps to do this include:

1. **Voluntary Storage:** The node voluntarily stores a blob copy even when not selected by the DHT's usual replication mechanism, typically motivated by a price hike or anticipated demand.
2. **Market Advertisement:** The node advertises its storage offer on the open market, specifying the terms, including the price and retrieval conditions.
3. **Availability Announcement:** The node updates its routing information and market listings to announce the availability of the stored blob, ensuring that other nodes in the network can discover and bid for retrieval services if needed.

### Market Dynamics

#### Incentive Structures

- **Quality of Service (QoS):** Nodes are incentivized to provide high-quality storage and retrieval services, which are reflected in their reputation. Higher reliability and faster access times may attract higher prices.
- **Reputation Impact:** Nodes failing to meet their promises face a degradation in their reputation, which discourages poor performance and broken promises.

#### Competitive Environment

- **Differentiation:** Nodes can differentiate themselves based on their QoS offerings, such as uptime, speed, and reliability.
- **Reputation Systems:** A robust reputation system tracks the performance and reliability of nodes, influencing their attractiveness and opportunities in the market.

### Security Considerations

#### Data Integrity and Privacy

- **Cryptographic Hashing:** Each data chunk is signed with a cryptographic hash to ensure integrity and detect tampering.
- **Encryption:** Data can be encrypted before storage to maintain privacy, with only authorized nodes having access to decryption keys.

#### Trust and Verification

- **Decentralized Trust:** The hybrid system minimizes the reliance on central authorities, relying instead on cryptographic promises and reputation systems to foster trust.
- **Auditability:** All transactions, promises, and broken promises are logged and auditable to ensure transparency and accountability.

### Transaction Logs

#### Storage of Transaction Logs

- **Hashed Blobs:** Transaction logs are themselves stored in the system as hashed blobs, just like any other data. This ensures their integrity, verifiability, and availability throughout the network.

### Conclusion

The hybrid DHT and open market design combines the decentralized robustness of DHT structures with the dynamic and competitive elements of an open market. By integrating these two paradigms, the system aims to enhance flexibility, scalability, and resilience in data storage and retrieval.

Future work will focus on refining the economic models, improving security mechanisms, and optimizing the performance of both the DHT backbone and the open market layer.
```
EOF_/home/stevegt/lab/grid-cli/v2/doc/003-design.md