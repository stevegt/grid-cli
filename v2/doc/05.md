# Module Integration and Advanced Concepts

## Advanced Discussions

This document consolidates advanced discussions from previous versions and integrates key learnings, strategies, and design considerations.

### Cache and Modules

The kernel MUST treat modules as caches. In the event of a cache miss, the kernel MUST consult one or more modules to retrieve the requested value.

1. **Cache Keys**:
    - The cache key MUST use filesystem separators (e.g., `/`) between each key component. Arguments MUST be URL-encoded when building the cache key to handle characters such as `/` that are unsafe in file or directory names.
    - Example: A cache key might be `promiseHash/moduleHash/arg1/arg2`.

2. **Consulting Modules as Caches**:
    - Modules act as part of the cache mechanism. On a cache miss, the kernel consults one or more modules to retrieve the requested value.
    - The cache lookup function takes multiple arguments: `promiseHash`, `moduleHash`, and zero or more arguments from the caller’s message.
    - If a value isn't in the cache, it's retrieved from the module(s), added to the cache, and returned.

### Cache Structures

In the context of the PromiseGrid Kernel, the cache plays a critical role in optimizing and managing the execution and storage of data. Here's a summary of the key points regarding the caching strategy:

1. **Cache Structures**:
   - The cache **SHOULD BE** a directory tree rather than an in-memory map.
   - Cache keys **MUST** use filesystem separators (`/`) between each key component and arguments **MUST** be URL-encoded when building the cache key, in order to safely handle characters in file or directory names.
   - There **MAY BE** multiple caches, including the built-in cache in the kernel and caches provided by various modules.

2. **Integration with Modules**:
   - The kernel **MUST** treat modules as caches. In the event of a cache miss, the kernel **MUST** consult one or more modules to retrieve the requested value.
   - The kernel **SHOULD** load the built-in cache from embedded resources using Go’s embed feature.
   - The kernel **MAY** use the Origin Private File System (OPFS) for disk file access and **MAY** utilize the afero library to abstract filesystem interactions.

3. **Unified Interface**:
   - From the caller's perspective, there **SHALL BE** no difference between a cache lookup and a function call. Both operations **SHALL BE** treated as hashed function calls. The caller sends a message with a leading hash and any arguments and receives a message containing the requested content.

### Kernel and Caching

#### Does the kernel have its own cache, or does it always rely on one or more modules for caching?

The kernel **SHOULD** maintain its own cache but **MUST** also treat modules as part of the caching mechanism. When the kernel encounters a cache miss, it **MUST** consult one or more modules to retrieve the requested value. This approach allows for a distributed and scalable caching solution where multiple modules can contribute to the cache.

### Network Communications

#### Are network communications always done through modules, or can the kernel do some of that itself?

Network communications in the PromiseGrid Kernel model are primarily intended to be handled by modules rather than the kernel itself. This design aligns with the microkernel architecture principles, where the kernel provides minimal core functionalities and delegates most tasks, such as network communication, to service modules. The kernel’s role includes managing module execution, handling inter-process communication (IPC), and maintaining security and resource control.

Integrating a module-based architecture for network communications ensures modularity, flexibility, and ease of updating or replacing network-related functionalities without altering the core kernel. However, the kernel **MAY** include basic communication capabilities for essential operations, but these would generally be minimal and supportive in nature, primarily to coordinate and manage module interactions.

### Ant Routing Mechanism: The Syscall Tree

The syscall tree acts like an "ant routing" mechanism:

1. **Hierarchy and Routing**:
    - The syscall tree uses hierarchical keys. Each node in the tree represents a level of parameter matching.
    - On a cache miss, the kernel routes the message to the module whose syscall tree key matches the most leading parameter components.
    - Example: If no exact match exists for `promiseHash/moduleHash/arg1/arg2`, but a node matches `promiseHash/moduleHash/arg1`, the kernel routes the message through that path.

2. **Acceptance as a Promise**:
    - Modules define an `Accept()` function that encompasses the acceptance criteria for promises, module hashes, and arguments.
    - The response from an `Accept()` call is considered a promise. If `HandleMessage()` fails after acceptance, it is considered a broken promise.

### Integration with Church, Turing, and Chomsky's Concept of "Accept"

1. **Computational Theory**:
    - Aligning acceptance with the concepts used by Church, Turing, and Chomsky. In computational theory, a machine or automaton accepts an input if it transitions into an accepting state.
    - Similarly, in PromiseGrid, modules act as recognizers or acceptors based on the promise hash, module hash, and arguments.

2. **Promises All the Way Down**:
    - The entire system leverages promises for each interaction layer. Acceptance and fulfillment of promises are managed consistently across modules, the syscall tree, and the kernel.

### Dynamic Syscall Table

The dynamic syscall table maintains positive and negative acceptance history:

1. **Populating the Syscall Table**:
    - The syscall table starts empty and populates during operation.
    - As the kernel consults built-in and other modules to handle messages, it updates the table based on which modules accept or reject specific parameter sets.

2. **Optimized Routing**:
    - On receiving a message, the kernel consults the syscall table to determine which modules to consult, optimizing message handling and reducing redundant checks.

### Example Implementations

#### Combined Function Approach

Combining decision-making and handling into a single function:

```go
type Module interface {
    ProcessMessage(ctx context.Context, parms ...interface{}) (Message, error)
}

type LocalCacheModule struct {
    cacheDir string
}

func NewLocalCacheModule(cacheDir string) *LocalCacheModule {
    return &LocalCacheModule{cacheDir: cacheDir}
}

func (m *LocalCacheModule) ProcessMessage(ctx context.Context, parms ...interface{}) (Message, error) {
    // Implement logic for processing message, including acceptance and handling
    // Return a promise message with acceptance and handling results or errors
}
```

#### Separate Functions Approach

Using distinct functions for acceptance and handling:

```go
type Module interface {
    Accept(ctx context.Context, parms ...interface{}) (Message, error)
    HandleMessage(ctx context.Context, parms ...interface{}) ([]byte, error)
}

type LocalCacheModule struct {
    cacheDir string
}

func NewLocalCacheModule(cacheDir string) *LocalCacheModule {
    return &LocalCacheModule{cacheDir: cacheDir}
}

func (m *LocalCacheModule) Accept(ctx context.Context, parms ...interface{}) (Message, error) {
    // Implement logic for accepting or rejecting based on parameters
}

func (m *LocalCacheModule) HandleMessage(ctx context.Context, parms ...interface{}) ([]byte, error) {
    // Implement logic for handling the message after acceptance
}
```

### Conclusion

By integrating promises at all levels and employing a hierarchical syscall tree with caching and acceptance history, PromiseGrid ensures trust, accountability, and efficient message handling. This structure supports a decentralized governance framework, robust system interactions, and reliable module functionality.

### JSON Web Tokens (JWT) in URLs

When it comes to encoding JSON Web Tokens (JWTs) for use in URLs, there's a standard format that is widely accepted. JWTs are typically encoded using Base64url, which is a URL-safe variant of Base64 encoding.

#### Why Base64url?

1. **URL-safe**: Base64url encoding replaces characters that are not URL-safe (such as `+`, `/`, and `=`) with URL-safe alternatives (`-`, `_`, and no padding). This ensures that the token can be safely included in a URL without requiring additional encoding.
2. **Human-readable**: While not as compact as some other encodings (like base58), Base64url maintains a balance between human readability and URL safety.
3. **Standardized**: The use of Base64url for JWTs is specified in the JSON Web Token (JWT) specification (RFC 7519). This standardization means that the approach is widely supported and understood, making it interoperable across different systems and libraries.

#### Encoding Steps

1. **Header and Payload**: The JWT consists of three parts: the header, the payload, and the signature. Each part is encoded separately using Base64url.
2. **Concatenation**: The three parts are concatenated together with periods (`.`) as separators. This forms the final JWT string that can be included in a URL.

#### Example

Suppose you have the following JWT:
```json
{
  "header": {
    "alg": "HS256",
    "typ": "JWT"
  },
  "payload": {
    "sub": "1234567890",
    "name": "John Doe",
    "iat": 1516239022
  },
  "signature": "SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c"
}
```

The encoded JWT would look something like this:
```
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c
```

This encoded JWT can be safely used as a URL parameter without further encoding.

Using the standard Base64url encoding for JWTs in URLs ensures compatibility with existing libraries and tools, making it a reliable choice for encoding JWT tokens for use in web applications.

## Capability Tokens and Prior Art

### Self-Contained Capability Tokens

A capability token can be crafted to be self-contained and self-describing by including both the permissions granted and the identity of the party to whom the permission is granted. This design ensures that the token itself carries all the information needed to understand the scope and target of the permissions without needing to reference external data.

### How It Works

1. **Structure**: The structure of such a self-contained capability token typically includes fields for:
   - **Identity of the Holder**: Information about the entity that holds the token.
   - **Permissions**: Details about what actions the holder is permitted to perform.
   - **Issuer’s Identity**: Information about the entity that issued the token.
   - **Signature**: Cryptographic signature from the issuer to ensure integrity and authenticity.

2. **Encoding**: The token can be encoded in a standardized format such as JSON Web Tokens (JWT) which supports custom claims to include the necessary fields.

3. **Verification**: When the token is presented, the system:
   - Verifies the cryptographic signature to ensure the token was indeed issued by the trusted party.
   - Checks the permissions described within the token.
   - Confirms the identity of the holder matches the intended recipient.


Several systems and frameworks implement self-contained capability tokens:

1. **Macaroons**: These are flexible authorization credentials that support delegation and attenuation. A macaroon is a bearer token that can encapsulate permissions and is augmented with caveats, which are conditions that must be satisfied for the macaroon to be considered valid.
2. **JSON Web Tokens (JWT)**: JWTs are widely used in web applications to assert claims between parties. They can include custom claims to specify permissions and the intended audience. A JWT can be signed to ensure authenticity and integrity.
3. **Caveats in Capability-Based Systems**: Traditional capability-based security systems sometimes support a form of caveats or restrictions within the tokens themselves to specify the scope of permissions and the authorized user.

### Recommendations

1. **Design Tokens Carefully**: Ensure that the structure of the tokens balances comprehensiveness with simplicity. Include necessary fields for permissions, identity, and signatures.
   
2. **Use Standard Formats**: Leverage existing standards like JWTs for encoding tokens to benefit from existing libraries and tools for creation, parsing, and verification.

3. **Cryptographic Security**: Ensure that tokens are signed using robust cryptographic methods to prevent forgery and tampering.

4. **Revocation**: Implement mechanisms to handle token revocation in case permissions need to be rescinded before the token naturally expires.

### Discussion on the Leading Hash in Messages

In the PromiseGrid Kernel, messages MAY start with a promise hash followed by a module hash, and then additional arguments. This structure allows receivers to filter messages based on promises they are willing to accept and route the message to the appropriate module. Here are the pros and cons of the promise hash coming first:

**Pros:**
- **Enhanced Filtering**: Placing the promise hash first allows modules and nodes to quickly filter messages based on the promises they accept.
- **Trust and Governance**: This aligns with decentralized governance. Nodes can establish trust relationships by agreeing on specific promises they will accept.
- **Modular Routing**: Early identification of the promise allows the kernel to route messages to the appropriate module, ensuring efficient distributed handling.

### Kernel's Dynamic Handling of Accept and HandleMessage

Implementing an advanced interaction model involves considering different versions of function handling of messages. There are two possible structures: combined functions or separate functions.

### Combined Function:

```go
type Module interface {
    ProcessMessage(ctx context.Context, parms ...interface{}) (Message, error)
}

type LocalCacheModule struct {
    cacheDir string
}

func NewLocalCacheModule(cacheDir string) *LocalCacheModule {
    return &LocalCacheModule{cacheDir: cacheDir}
}

func (m *LocalCacheModule) ProcessMessage(ctx context.Context, parms ...interface{}) (Message, error) {
    // Implement logic to process message, which includes accepting and handling
    // Return a promise message with acceptance and handling results or errors
}
```

### Separate Functions:

```go
type Module interface {
    Accept(ctx context.Context, parms ...interface{}) (Message, error)
    HandleMessage(ctx context.Context, parms ...interface{}) ([]byte, error)
}

type LocalCacheModule struct {
    cacheDir string
}

func NewLocalCacheModule(cacheDir string) *LocalCacheModule {
    return &LocalCacheModule{cacheDir: cacheDir}
}

func (m *LocalCacheModule) Accept(ctx context.Context, parms ...interface{}) (Message, error) {
   // Implement logic for accepting or rejecting based on parameters
}

func (m *LocalCacheModule) HandleMessage(ctx context.Context, parms ...interface{}) ([]byte, error) {
    // Implement logic for handling the message after acceptance
}
```

### Conclusion

By integrating promises at every level and implementing a hierarchical syscall tree with caching and acceptance history, PromiseGrid ensures trust, accountability, and efficient message handling in a decentralized governance framework. The simplified message structure and consistent handling of cache and modules contribute to a robust and flexible system.

### Importance of Including Module Hash in Messages for Deterministic Execution and Demand-Driven Deployment of New Code

Including the hash of the module to be executed in the message is crucial for achieving deterministic execution and facilitating demand-driven deployment of new code in decentralized systems. Here are key reasons why this practice is recommended:

1. **Deterministic Execution**:
   Including the module hash in the message ensures that the same piece of code is executed consistently across different nodes. This deterministic behavior is essential for maintaining consistency and correctness in distributed systems. By specifying the exact module to be run, all nodes can reliably and repeatably execute the same logic, avoiding differences in behavior due to variations in module versions or implementations.

2. **Content-Addressable Code**:
   The module hash serves as a unique identifier for the specific version of the code to be executed. This content-addressable storage mechanism ensures that the precise code is located and executed, eliminating ambiguity. Nodes can fetch the exact module specified by the hash, even if new versions or updates are available, ensuring backward compatibility and preventing unexpected changes in system behavior.

3. **Demand-Driven Deployment**:
   By including the module hash in the message, new code can be deployed on-demand. Nodes that do not have the required module can fetch it dynamically based on the hash, allowing for seamless updates and scaling. This demand-driven deployment model ensures that nodes always execute the latest or specified version of the module, promoting flexibility and ease of maintenance in decentralized environments.

4. **Security and Trust**:
   The module hash provides a way to verify the integrity and authenticity of the code being executed. Nodes can validate the fetched module against the specified hash, ensuring that the code has not been tampered with. This practice builds trust among nodes, as they can rely on the exact code promised in the message without the risk of executing malicious or unauthorized code.

5. **Enabling Capabilities**:
   Including module hashes in messages aligns with the capability-based security model. Capabilities can include specific module hashes that nodes are authorized to execute, providing fine-grained access control. This approach ensures that nodes only execute permitted code, enhancing the overall security and governance of the decentralized system.

In summary, including the module hash in messages is essential for achieving deterministic execution, enabling content-addressable storage, facilitating demand-driven deployment of new code, ensuring security, and supporting capability-based access control. This practice aligns with the principles of decentralized systems, promoting consistency, flexibility, and trust among participating nodes.

## Supercomputer and Mainframe Systems with Hot-Swappable Modules

### Overview

Several advanced supercomputer and mainframe systems utilize hot-swappable, upgradeable modules that advertise their own capabilities and requirements upon insertion and self-describe their degraded modes when they encounter failures. These systems dynamically reconfigure themselves to accommodate new modules and can detect and respond to their removal.

### Examples of Systems

1. **IBM zSeries Mainframes**:
    - **Modular Design**: IBM's zSeries mainframes, such as the z14 and z15, use modular components for CPUs, memory, and I/O subsystems. These components are designed to be hot-swappable.
    - **Dynamic Reconfiguration**: The system can dynamically reconfigure itself when a new module is added. The new module broadcasts its capabilities (e.g., processing power, memory capacity) to the system.
    - **Failure Management**: When a module fails, it advertises its degraded state, and the system responds by rerouting tasks to operational modules. This ensures continuous operation without significant performance degradation.
    - **Capabilities and Requirements**: Upon insertion, modules advertise their capabilities and requirements, such as power consumption and cooling needs, enabling the system to adjust resource allocation dynamically.

2. **Cray XC Series Supercomputers**:
    - **Hot-Swappable Blades**: Cray's XC series supercomputers use hot-swappable blades for computational and networking tasks.
    - **Dynamic System Configuration**: The system's software layer dynamically recognizes and configures new blades, integrating them into the computational framework seamlessly.
    - **Degraded Mode Advertisement**: Components in the Cray XC series automatically report failures and their new operational status. The system adjusts workloads and schedules maintenance as needed.
    - **Resource Advertisement**: Each blade communicates its processing capabilities and networking requirements upon connection, facilitating optimal system performance and resource utilization.

3. **HPE Superdome Flex**:
    - **Upgradeable Modules**: HPE's Superdome Flex servers support hot-swappable modules for CPUs, memory, and I/O, allowing for easy upgrades and maintenance.
    - **Self-Configuration**: The system automatically recognizes new modules, integrates their capabilities, and reconfigures to maximize efficiency.
    - **Fault Tolerance**: When a module fails, it signals its degraded mode to the system. Remaining modules redistribute tasks and maintain service continuity.
    - **Capability Advertisement**: Modules advertise their capabilities (e.g., compute power, memory size) and resource requirements, allowing the system to balance loads and allocate resources effectively.

### Dynamic Reconfiguration and Removal Detection

- **Insertion and Self-Advertisement**: When a new module is inserted, it sends a message to the system controller, advertising its capabilities, such as processing speed, memory size, and other resources. It also specifies its operational requirements, like power and cooling needs.
- **Dynamic System Reconfiguration**: The system controller dynamically updates the system configuration to utilize the new module fully. This might involve redistributing workloads, updating internal routing tables, or reallocating memory.
- **Degraded Modes and Failure Handling**: If a module fails, it sends a degraded mode message to the system controller, describing its reduced capabilities. The controller then reroutes tasks from the failed module to other operational modules to maintain performance and reliability.
- **Module Removal Detection**: When a module is removed, it signals its disengagement to the system controller. The controller then adjusts the system configuration to operate without the module, redistributing tasks and resources as needed to prevent service interruption.

### Conclusion

The use of hot-swappable, upgradeable modules in supercomputer and mainframe systems like IBM zSeries, Cray XC, and HPE Superdome Flex highlights the importance of dynamic reconfiguration and real-time failure management. These capabilities ensure high availability, scalability, and efficient resource utilization, making them essential for modern computational requirements.

## Deferred System Comparison

### Hot-Swappable, Upgradeable Modules

PromiseGrid modules are analogous to systems used in high-reliability environments like supercomputers or mainframe systems. These systems employ hot-swappable, upgradeable modules designed for continuous operation and flexibility. Here’s how PromiseGrid aligns with these concepts:

1. **Module Capabilities and Requirements**:
    - In supercomputers and mainframe systems, modules advertise their capabilities and requirements upon insertion.
    - Similarly, PromiseGrid modules advertise their own capabilities (e.g., functions they can perform) and requirements (e.g., resources needed) upon activation or registration within the grid.

2. **Degraded Modes and Failure Management**:
    - High-reliability systems can continue operating in a degraded mode when a module fails, advertising this degraded state for system-wide awareness.
    - PromiseGrid modules also advertise their degraded modes in case of failure, allowing the grid to adjust dynamically. This ensures that the grid remains operational even when parts of it are in failure.

3. **Dynamic Reconfiguration**:
    - Supercomputers and mainframe systems support dynamic reconfiguration to integrate or remove modules without shutting down.
    - PromiseGrid supports dynamic module registration and deregistration, allowing the grid to adapt to changes in real-time without requiring a complete restart.

4. **Hot-Swapping**:
    - Hot-swapping involves replacing or adding modules to a system without interrupting its operation.
    - PromiseGrid allows the dynamic inclusion of modules, enabling seamless updates and scaling without disrupting ongoing processes.

### Governance and Trust

The concept of modules advertising their capabilities and degraded states upon failure in PromiseGrid ties into broader themes of governance and trust:

1. **Transparency**: Modules providing detailed capability and status information promotes transparency within the grid.
2. **Trust**: Trust relationships are reinforced as modules self-report their state, ensuring that the grid can rely on accurate, real-time information for decision-making.
3. **Autonomy**: Modules operate autonomously but within the framework of the grid, supporting decentralized governance by allowing modules to self-manage and report.

By integrating these concepts, PromiseGrid aligns with the principles of high-reliability systems while fostering a robust, self-governing, and adaptable decentralized computation grid.
